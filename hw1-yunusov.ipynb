{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/\r\n",
      "data/eval.py\r\n",
      "data/cran.qry\r\n",
      "data/qrel_clean\r\n",
      "data/cran.all.1400\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xvzf data/data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".I 1\r\n",
      ".T\r\n",
      "experimental investigation of the aerodynamics of a\r\n",
      "wing in a slipstream .\r\n",
      ".A\r\n",
      "brenckman,m.\r\n",
      ".B\r\n",
      "j. ae. scs. 25, 1958, 324.\r\n",
      ".W\r\n",
      "experimental investigation of the aerodynamics of a\r\n",
      "wing in a slipstream .\r\n",
      "  an experimental study of a wing in a propeller slipstream was\r\n",
      "made in order to determine the spanwise distribution of the lift\r\n",
      "increase due to slipstream at different angles of attack of the wing\r\n",
      "and at different free stream to slipstream velocity ratios .  the\r\n",
      "results were intended in part as an evaluation basis for different\r\n",
      "theoretical treatments of this problem .\r\n",
      "  the comparative span loading curves, together with\r\n",
      "supporting evidence, showed that a substantial part of the lift increment\r\n",
      "produced by the slipstream was due to a /destalling/ or\r\n",
      "boundary-layer-control effect .  the integrated remaining lift\r\n",
      "increment, after subtracting this destalling lift, was found to agree\r\n",
      "well with a potential flow theory .\r\n",
      "  an empirical evaluation of the destalling effects was made for\r\n",
      "the specific configuration of the experiment .\r\n",
      ".I 2\r\n",
      ".T\r\n",
      "simple shear flow past a flat plate in an incompressible fluid of small\r\n",
      "viscosity .\r\n",
      ".A\r\n"
     ]
    }
   ],
   "source": [
    "!head -30 data/cran.all.1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from itertools import (\n",
    "    chain\n",
    ")\n",
    "\n",
    "from typing import (\n",
    "    Optional,\n",
    "    List\n",
    ")\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "ALL_DATA_FILE = os.path.join(BASE_DIR, \"cran.all.1400\")\n",
    "NUMBER_TEXTS = 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text:\n",
    "    __slots__ = [\"i\", \"t\", \"a\", \"b\", \"w\"]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.i = None # type: str\n",
    "        self.t = None # type: str\n",
    "        self.a = None # type: str\n",
    "        self.b = None # type: str\n",
    "        self.w = None # type: str\n",
    "\n",
    "\n",
    "def _read_file(filepath):\n",
    "    # type: (str) -> Optional[List[str]]\n",
    "    \n",
    "    with open(filepath, \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        return lines\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def _parse_text(lines):\n",
    "    # type: (str) -> List[Text]\n",
    "    \n",
    "    def set_current_state(line):\n",
    "        nonlocal current_state\n",
    "        \n",
    "        for i, (s, _) in enumerate(line_starts):\n",
    "            if line.startswith(s):\n",
    "                current_state = i\n",
    "    \n",
    "    def add_text():\n",
    "        nonlocal texts\n",
    "\n",
    "        t = Text()\n",
    "        for i, (_, s) in enumerate(line_starts):\n",
    "            setattr(t, s, ''.join(text_lists[i]))\n",
    "        \n",
    "        texts.append(t)\n",
    "    \n",
    "    line_starts = [('.I', 'i'), ('.T', 't'), ('.A', 'a'), ('.B', 'b'), ('.W', 'w')]\n",
    "    text_lists, texts = [[] for _ in range(len(line_starts))], []\n",
    "    current_state = -1\n",
    "\n",
    "    for line in chain(lines, ['.I']):\n",
    "        set_current_state(line)\n",
    "        \n",
    "        if current_state == 0:\n",
    "            if any(text_lists):\n",
    "                add_text()\n",
    "            texts_lines = [[] for _ in range(len(line_starts))]\n",
    "        \n",
    "        text_lists[current_state].append(line)\n",
    "    \n",
    "    return texts\n",
    "\n",
    "\n",
    "def get_texts(filename):\n",
    "    # type: (str) -> List[Text]\n",
    "\n",
    "    data = _read_file(filename)\n",
    "    \n",
    "    assert data != None\n",
    "    \n",
    "    texts = _parse_text(data)\n",
    "    return texts\n",
    "\n",
    "\n",
    "texts = get_texts(ALL_DATA_FILE)\n",
    "assert len(texts) == NUMBER_TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextTokens:\n",
    "    __slots__ = [\"i\", \"tokens\"]\n",
    "    \n",
    "    def __init__(self, text, attr):\n",
    "        self.i = text.i\n",
    "        \n",
    "        assert hasattr(text, attr)\n",
    "        \n",
    "        self.tokens = self._clean(getattr(text, attr))\n",
    "    \n",
    "    def _clean(self, s):\n",
    "        return (t.lemma_ for t in tokenizer(s))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.tokens\n",
    "\n",
    "    \n",
    "def get_text_tokens_gen(texts, attr):\n",
    "    # type: (List[Text], str) -> List[Tokens]\n",
    "    \n",
    "    text_tokens = (TextTokens(text, attr) for text in texts)\n",
    "    return text_tokens\n",
    "\n",
    "\n",
    "text_tokens_gen = get_text_tokens_gen(texts, 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "821it [04:43,  2.90it/s]"
     ]
    }
   ],
   "source": [
    "class InvIndex:\n",
    "    def __init__(self, text_tokens_gen):\n",
    "        data = [(token, doc_id, 1) for doc_id, text_tokens in tqdm(enumerate(text_tokens_gen), miniters=40) for token in text_tokens]\n",
    "        self.df = DataFrame(data, columns=[\"token\", \"doc_id\", \"count\"])\n",
    "\n",
    "inv_index = InvIndex(text_tokens_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
