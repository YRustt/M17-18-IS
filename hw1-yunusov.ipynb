{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо положить `data.tar.gz` в директорию `data`, а потом подменить `eval.py` на исправленный вариант."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -xvzf data/data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".I 1\r\n",
      ".T\r\n",
      "experimental investigation of the aerodynamics of a\r\n",
      "wing in a slipstream .\r\n",
      ".A\r\n",
      "brenckman,m.\r\n",
      ".B\r\n",
      "j. ae. scs. 25, 1958, 324.\r\n",
      ".W\r\n",
      "experimental investigation of the aerodynamics of a\r\n",
      "wing in a slipstream .\r\n",
      "  an experimental study of a wing in a propeller slipstream was\r\n",
      "made in order to determine the spanwise distribution of the lift\r\n",
      "increase due to slipstream at different angles of attack of the wing\r\n",
      "and at different free stream to slipstream velocity ratios .  the\r\n",
      "results were intended in part as an evaluation basis for different\r\n",
      "theoretical treatments of this problem .\r\n",
      "  the comparative span loading curves, together with\r\n",
      "supporting evidence, showed that a substantial part of the lift increment\r\n",
      "produced by the slipstream was due to a /destalling/ or\r\n",
      "boundary-layer-control effect .  the integrated remaining lift\r\n",
      "increment, after subtracting this destalling lift, was found to agree\r\n",
      "well with a potential flow theory .\r\n",
      "  an empirical evaluation of the destalling effects was made for\r\n",
      "the specific configuration of the experiment .\r\n",
      ".I 2\r\n",
      ".T\r\n",
      "simple shear flow past a flat plate in an incompressible fluid of small\r\n",
      "viscosity .\r\n",
      ".A\r\n"
     ]
    }
   ],
   "source": [
    "!head -30 data/cran.all.1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import abc\n",
    "import math\n",
    "import functools\n",
    "import string\n",
    "from itertools import chain\n",
    "from operator import itemgetter\n",
    "\n",
    "from typing import (\n",
    "    Optional,\n",
    "    Generator,\n",
    "    List\n",
    ")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "TEXTS_FILE = os.path.join(BASE_DIR, \"cran.all.1400\")\n",
    "QUERIES_FILE = os.path.join(BASE_DIR, \"cran.qry\")\n",
    "CORRECT_ANSWERS_FILE = os.path.join(BASE_DIR, \"test.qrel_clean\")\n",
    "PREDICTION_FILE = os.path.join(BASE_DIR, \"train.qrel_clean\")\n",
    "NUMBER_TEXTS = 1400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Text:\n",
    "    __slots__ = [\"i\", \"t\", \"a\", \"b\", \"w\"]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.i = None # type: str\n",
    "        self.t = None # type: str\n",
    "        self.a = None # type: str\n",
    "        self.b = None # type: str\n",
    "        self.w = None # type: str\n",
    "\n",
    "        \n",
    "class Query:\n",
    "    __slots__ = [\"i\", \"w\"]\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.i = None # type: str\n",
    "        self.w = None # type: str\n",
    "\n",
    "\n",
    "def _read_file(filepath):\n",
    "    # type: (str) -> str\n",
    "    \n",
    "    file = open(filepath)\n",
    "    yield from file\n",
    "        \n",
    "\n",
    "def _parse(gen, cls, line_starts):\n",
    "    # type: (Generator[str]) -> Text\n",
    "    \n",
    "    def set_current_state(line):\n",
    "        nonlocal current_state\n",
    "        \n",
    "        for i, (s, _) in enumerate(line_starts):\n",
    "            if line.startswith(s):\n",
    "                current_state = i\n",
    "    \n",
    "    def yield_text():\n",
    "        t = cls()\n",
    "        for i, (_, s) in enumerate(line_starts):\n",
    "            setattr(t, s, ''.join(text_lists[i]))\n",
    "        \n",
    "        return t\n",
    "    \n",
    "    text_lists = [[] for _ in range(len(line_starts))]\n",
    "    current_state = -1\n",
    "\n",
    "    for line in chain(gen, ['.I']):\n",
    "        set_current_state(line)\n",
    "        \n",
    "        if current_state == 0:\n",
    "            if any(text_lists):\n",
    "                yield yield_text()\n",
    "            text_lists = [[] for _ in range(len(line_starts))]\n",
    "        \n",
    "        text_lists[current_state].append(line)\n",
    "\n",
    "\n",
    "def get_texts_gen(filepath):\n",
    "    # type: (str) -> Generator[Text]\n",
    "\n",
    "    gen = _read_file(filepath)\n",
    "    texts = _parse(gen, Text, [('.I', 'i'), ('.T', 't'), ('.A', 'a'), ('.B', 'b'), ('.W', 'w')])\n",
    "    return texts\n",
    "\n",
    "\n",
    "def get_queries_gen(filepath):\n",
    "    # type: (str) -> Generator[Query]\n",
    "    \n",
    "    gen = _read_file(filepath)\n",
    "    queries = _parse(gen, Query, [('.I', 'i'), ('.W', 'w')])\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemTokens:\n",
    "    __slots__ = [\"i\", \"tokens\"]\n",
    "    \n",
    "    def __init__(self, text, attr):\n",
    "        self.i = text.i\n",
    "        \n",
    "        assert hasattr(text, attr)\n",
    "        \n",
    "        self.tokens = self._filter(self._clean(getattr(text, attr)))\n",
    "    \n",
    "    def _clean(self, s):\n",
    "        for t in string.punctuation:\n",
    "            s = s.replace(t, \" \")\n",
    "        return s\n",
    "    \n",
    "    def _filter(self, s):\n",
    "        stop_tokens = [\"I\", \"T\", \"A\", \"B\", \"W\"]\n",
    "        return (stemmer.stem(lemmatizer.lemmatize(t)) for t in word_tokenize(s) \n",
    "                if (t not in stop_words) and (t not in stop_tokens) and (t not in string.punctuation) \n",
    "                and (not t.isdigit()))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.tokens\n",
    "\n",
    "    \n",
    "def get_item_tokens_gen(texts, attr):\n",
    "    # type: (List[Text], str) -> Generator[TextTokens]\n",
    "    \n",
    "    item_tokens = (ItemTokens(text, attr) for text in texts)\n",
    "    return item_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvIndex:\n",
    "    def __init__(self, text_tokens_gen):\n",
    "        columns = [\"doc_id\", \"token\", \"count\"]\n",
    "        index = [\"doc_id\", \"token\"]\n",
    "        \n",
    "        def get_part_df():\n",
    "            for doc_id, text_tokens in tqdm(enumerate(text_tokens_gen)):\n",
    "                data = [(doc_id, token, 1) for token in text_tokens]\n",
    "                df = DataFrame(data, columns=columns).groupby(by=index).sum()\n",
    "                yield df\n",
    "        \n",
    "        self.df = pd.concat(get_part_df())\n",
    "        self.df[\"count\"] = self.df[\"count\"].astype(np.float32)\n",
    "    \n",
    "    @functools.lru_cache(maxsize=256, typed=False)\n",
    "    def get_n(self, t=None):\n",
    "        try:\n",
    "            if t is None:\n",
    "                return self.df[\"count\"].sum()\n",
    "\n",
    "            return self.df.loc[(slice(None), t), :][\"count\"].sum()\n",
    "        except KeyError:\n",
    "            return 0\n",
    "    \n",
    "    @functools.lru_cache(maxsize=256, typed=False)\n",
    "    def get_f(self, t, doc_id=None):\n",
    "        try:\n",
    "            if doc_id is not None:\n",
    "                return self.df.loc[(doc_id, t), :]\n",
    "            \n",
    "            return self.df.loc[(slice(None), t), :]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    @functools.lru_cache(maxsize=256, typed=False)\n",
    "    def get_l(self, doc_id=None):\n",
    "        try:\n",
    "            if doc_id is not None:\n",
    "                return self.df.loc[doc_id, :][\"count\"].sum()\n",
    "            \n",
    "            return self.df.reset_index().groupby(\"doc_id\").sum()[\"count\"].mean()\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankedList:\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def __call__(self, q, inv_index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class RSVRankedList(RankedList):\n",
    "    def __init__(self, k1, b):\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "    \n",
    "    def __call__(self, q, inv_index):\n",
    "        rsv = {}\n",
    "        N = inv_index.get_n()\n",
    "        \n",
    "        for t in q:\n",
    "            Nt = inv_index.get_n(t)\n",
    "            F = inv_index.get_f(t)\n",
    "            idf = math.log(1.0 + (N - Nt + 0.5) / (Nt + 0.5))\n",
    "            \n",
    "            if F is not None:\n",
    "                for index, row in F.iterrows():\n",
    "                    doc_id, ftd = index[0], row[\"count\"]\n",
    "                    Ld, L = inv_index.get_l(doc_id), inv_index.get_l()\n",
    "                    tf = ftd * (self.k1 + 1.) / (self.k1 * ((1. - self.b) + self.b * Ld / L) + ftd)\n",
    "                    rsv[doc_id] = rsv.get(doc_id, 0) + idf * tf\n",
    "        return sorted(rsv.items(), key=itemgetter(1), reverse=True)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1400it [00:12, 113.70it/s]\n",
      "225it [03:28,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "texts_gen = get_texts_gen(TEXTS_FILE)\n",
    "text_tokens_gen = get_item_tokens_gen(texts_gen, 'w')\n",
    "rsv = RSVRankedList(k1=1.2, b=0.75)\n",
    "\n",
    "queries_gen = get_queries_gen(QUERIES_FILE)\n",
    "query_tokens_gen = get_item_tokens_gen(queries_gen, 'w')\n",
    "\n",
    "inv_index = InvIndex(text_tokens_gen)\n",
    "with open(PREDICTION_FILE, \"w\") as fout:\n",
    "    for query_id, query in tqdm(enumerate(query_tokens_gen)):\n",
    "        ranked_list = rsv(query, inv_index)\n",
    "        for doc_id, _ in ranked_list:\n",
    "            fout.write(\"{} {}\\n\".format(query_id + 1, doc_id + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean precision: 0.29155555555555557\n",
      "mean recall: 0.42536748678214453\n",
      "mean F-measure: 0.3459736864354289\n",
      "MAP@10: 0.3569053966854231\n"
     ]
    }
   ],
   "source": [
    "!cd data && python3 eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index.df.to_csv(\"data/df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index.df.sort_values(by=\"count\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
