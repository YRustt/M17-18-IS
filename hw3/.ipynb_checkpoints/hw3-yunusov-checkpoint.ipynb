{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "from urllib import parse\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict\n",
    "\n",
    "import scrapy\n",
    "from scrapy.spiders import Spider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "logging.getLogger('scrapy').propagate = False\n",
    "\n",
    "\n",
    "BASE_DIR = \"data\"\n",
    "URLS_FILE = os.path.join(BASE_DIR, \"urlid.csv\")\n",
    "\n",
    "SCRAPY_RESULT_FILE = os.path.join(BASE_DIR, \"docs.json\")\n",
    "BASE_URL = \"https://ru.wikipedia.org\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(filename: str, full=False):\n",
    "    with open(filename) as fin:\n",
    "        for line in fin:\n",
    "            idx, url = line.strip().split(\",\", 1)\n",
    "            url = parse.urljoin(BASE_URL, url)\n",
    "            yield (idx, url) if full else url\n",
    "\n",
    "\n",
    "class CustomSpider(Spider):\n",
    "    name = \"custom_spider\"\n",
    "    \n",
    "    def start_requests(self):\n",
    "        start_urls = get_urls(URLS_FILE)\n",
    "        for url in start_urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "    \n",
    "    def parse(self, response):\n",
    "        try:\n",
    "            soup = BeautifulSoup(response.body, \"lxml\")\n",
    "            title = soup.find(\"h1\", {\"id\": \"firstHeading\"}).text\n",
    "            div_content = soup.find(\"div\", {\"id\": \"mw-content-text\"})\n",
    "            snippet = \" \".join(p.text for p in div_content.find_all(\"p\"))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return {'url': response.url, 'title': locals().get('title', ''), 'snippet': locals().get('snippet', '')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': SCRAPY_RESULT_FILE\n",
    "})\n",
    "process.crawl(CustomSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_json = json.load(open(SCRAPY_RESULT_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_json)k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Doc:\n",
    "    __slots__ = (\"id\", \"url\", \"title\", \"snippet\")\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.id = None # type: int\n",
    "        self.url = None # type: str\n",
    "        self.title = None # type: str\n",
    "        self.snippet = None # type: str\n",
    "\n",
    "\n",
    "class Query:\n",
    "    __slots__ = (\"id\", \"text\")\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.id = None # type: int\n",
    "        self.text = None # type: str\n",
    "\n",
    "\n",
    "def get_doc_gen():\n",
    "    urls = {url: idx for url, idx in get_urls(URLS_FILE, full=True)}\n",
    "    json_data = json.load(SCRAPY_RESULT_FILE)\n",
    "    for doc in json_data:\n",
    "        d = Doc()\n",
    "        d.url = doc['url']\n",
    "        d.title = doc['title']\n",
    "        d.snippet = doc['snippet']\n",
    "        d.id = urls[d.url]\n",
    "        yield d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ItemTokens:\n",
    "    __slots__ = [\"i\", \"tokens\"]\n",
    "    \n",
    "    def __init__(self, text, attr):\n",
    "        self.i = text.i\n",
    "        \n",
    "        assert hasattr(text, attr)\n",
    "        \n",
    "        self.tokens = self._filter(self._clean(getattr(text, attr)))\n",
    "    \n",
    "    def _clean(self, s):\n",
    "        for t in chain(string.punctuation, string.digits):\n",
    "            s = s.replace(t, \" \")\n",
    "        return s\n",
    "    \n",
    "    def _filter(self, s):\n",
    "        stop_tokens = [\"I\", \"T\", \"A\", \"B\", \"W\"]\n",
    "        return (stemmer.stem(lemmatizer.lemmatize(t)) for t in word_tokenize(s) \n",
    "                if (t not in stop_words) and (t not in stop_tokens) and (len(t) >= 2))\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return self.tokens\n",
    "\n",
    "    \n",
    "def get_item_tokens_gen(texts, attr):\n",
    "    # type: (List[Text], str) -> Generator[TextTokens]\n",
    "    \n",
    "    item_tokens = (ItemTokens(text, attr) for text in texts)\n",
    "    return item_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvIndex:\n",
    "    def __init__(self, text_tokens_gen):\n",
    "        columns = [\"doc_id\", \"token\", \"count\"]\n",
    "        index = [\"doc_id\", \"token\"]\n",
    "        \n",
    "        def get_part_df():\n",
    "            for doc_id, text_tokens in tqdm(enumerate(text_tokens_gen)):\n",
    "                data = [(doc_id, token, 1) for token in text_tokens]\n",
    "                df = DataFrame(data, columns=columns).groupby(by=index).sum()\n",
    "                yield df\n",
    "        \n",
    "        self.df = pd.concat(get_part_df())\n",
    "        self.df[\"count\"] = self.df[\"count\"].astype(np.float32)\n",
    "    \n",
    "    @functools.lru_cache(maxsize=256, typed=False)\n",
    "    def get_n(self, t=None):\n",
    "        try:\n",
    "            if t is None:\n",
    "                return self.df[\"count\"].sum()\n",
    "\n",
    "            return self.df.loc[(slice(None), t), :][\"count\"].sum()\n",
    "        except KeyError:\n",
    "            return 0\n",
    "    \n",
    "    @functools.lru_cache(maxsize=256, typed=False)\n",
    "    def get_f(self, t, doc_id=None):\n",
    "        try:\n",
    "            if doc_id is not None:\n",
    "                return self.df.loc[(doc_id, t), :]\n",
    "            \n",
    "            return self.df.loc[(slice(None), t), :]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    @functools.lru_cache(maxsize=256, typed=False)\n",
    "    def get_l(self, doc_id=None):\n",
    "        try:\n",
    "            if doc_id is not None:\n",
    "                return self.df.loc[doc_id, :][\"count\"].sum()\n",
    "            \n",
    "            return self.df.reset_index().groupby(\"doc_id\").sum()[\"count\"].mean()\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
