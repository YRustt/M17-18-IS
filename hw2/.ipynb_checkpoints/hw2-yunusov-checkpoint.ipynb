{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('scrapy').propagate = False\n",
    "\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "import scrapy.crawler as crawler\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "SCRAPY_RESULT_FILE = \"graph.json\"\n",
    "SCRAPY_PAGES_COUNT = 10000\n",
    "SCRAPY_LINKS_LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpider(CrawlSpider):\n",
    "    name = \"custom_spider\"\n",
    "    \n",
    "    allowed_domains = [\"wikipedia.org\"]\n",
    "    start_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Information_retrieval\",\n",
    "        \"https://en.wikipedia.org/wiki/Mein_Kampf\",\n",
    "        \"https://en.wikipedia.org/wiki/Soviet_Union\",\n",
    "        \"https://en.wikipedia.org/wiki/Nineteen_Eighty-Four\",\n",
    "        \"https://en.wikipedia.org/wiki/The_Hero_with_a_Thousand_Faces\",\n",
    "    ]\n",
    "    rules = (\n",
    "        Rule(LinkExtractor(allow=\"https://.+\\.wikipedia\\.org/wiki/\" + \\\n",
    "                                 \"(?!(File|Talk|Category|Portal|Special|Wikipedia|Help|Draft|Main_Page)).+\",\n",
    "                           restrict_xpaths='//div[@id=\"mw-content-text\"]',\n",
    "                           canonicalize=True,\n",
    "                           unique=True),\n",
    "             process_links=lambda links: links[:SCRAPY_LINKS_LIMIT],\n",
    "             callback=\"parse_item\", \n",
    "             follow=True),\n",
    "    )\n",
    "    \n",
    "    custom_settings = {\n",
    "        \"CLOSESPIDER_PAGECOUNT\": SCRAPY_PAGES_COUNT,\n",
    "        \"CLOSESPIDER_ERRORCOUNT\": 0,\n",
    "        \"CONCURRENT_REQUESTS\": 16\n",
    "    }\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        try:\n",
    "            title = response.css('h1#firstHeading.firstHeading::text').extract_first()\n",
    "            snippet = BeautifulSoup(response.css('p').extract_first(), \"lxml\").text[:255] + \"...\"\n",
    "            links = [lnk.url for rule in self._rules \n",
    "                     for lnk in rule.process_links(rule.link_extractor.extract_links(response))]\n",
    "            return {'url': response.url, 'title': title, 'snippet': snippet, 'links': links}\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = crawler.CrawlerProcess({\n",
    "    'FEED_FORMAT': 'json',\n",
    "    'FEED_URI': SCRAPY_RESULT_FILE\n",
    "})\n",
    "runner.crawl(CustomSpider)\n",
    "runner.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_json = json.load(open(SCRAPY_RESULT_FILE), encoding='utf-8')\n",
    "G = nx.DiGraph()\n",
    "\n",
    "nodes = {x[\"url\"] for x in graph_json}\n",
    "def get_edges():\n",
    "\n",
    "    for line in graph_json:\n",
    "        source, targets = line[\"url\"], line[\"links\"]\n",
    "\n",
    "        for target in targets:\n",
    "            if target in nodes:\n",
    "                yield source, target\n",
    "\n",
    "\n",
    "G.add_edges_from(get_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 9845\n",
      "Number of edges: 40684\n",
      "Max out degree: 21\n",
      "Max in degree: 456\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of nodes: %d\" % G.number_of_nodes())\n",
    "print(\"Number of edges: %d\" % G.number_of_edges())\n",
    "print(\"Max out degree: %d\" % max(deg for _, deg in G.out_degree(nodes)))\n",
    "print(\"Max in degree: %d\" % max(deg for _, deg in G.in_degree(nodes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max pagerank: 0.033433\n",
      "Min pagerank: 0.000017\n"
     ]
    }
   ],
   "source": [
    "pagerank = nx.pagerank(G)\n",
    "print(\"Max pagerank: %f\" % max(pagerank.values()))\n",
    "print(\"Min pagerank: %f\" % min(pagerank.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m Integrated Authority File 0.03343322123641936 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Integrated_Authority_File\n",
      "The Integrated Authority File (German: Gemeinsame Normdatei, also known as: Universal Authority File) or GND is an international authority file for the organisation of personal names, subject headings and corporate bodies from catalogues. It is used mainl...\n",
      "\n",
      "\u001b[95m\u001b[1m IMDb 0.019211378948140727 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/IMDb\n",
      "IMDb, also known as Internet Movie Database, is an online database of information related to world films, television programs, home videos and video games, and internet streams, including cast, production crew, personnel and fictional character biographie...\n",
      "\n",
      "\u001b[95m\u001b[1m CBS 0.004939255254926932 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/CBS\n",
      "CBS (an initialism of the network's former name, the Columbia Broadcasting System) is an American English language commercial broadcast television network that is a flagship property of CBS Corporation. The company is headquartered at the CBS Building in ...\n",
      "\n",
      "\u001b[95m\u001b[1m NASA 0.004628257184426797 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/NASA\n",
      "The National Aeronautics and Space Administration (NASA /ˈnæsə/) is an independent agency of the executive branch of the United States federal government responsible for the civilian space program, as well as aeronautics and aerospace research.[note 1]...\n",
      "\n",
      "\u001b[95m\u001b[1m ISO 4 0.004097108638241366 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/ISO_4\n",
      "ISO 4 (Information and documentation – Rules for the abbreviation of title words and titles of publications) is an international standard which defines a uniform system for the abbreviation of serial titles, i.e., titles of publications such as scientific...\n",
      "\n",
      "\u001b[95m\u001b[1m New Zealand 0.004075282838729097 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/New_Zealand\n",
      "New Zealand (/njuːˈziːlənd/ ( listen); Māori: Aotearoa [aɔˈtɛaɾɔa]) is an island country in the southwestern Pacific Ocean. The country geographically comprises two main landmasses—the North Island (Te Ika-a-Māui), and the South Island (Te Waipounamu)—and...\n",
      "\n",
      "\u001b[95m\u001b[1m Bibsys 0.00363400212793646 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/BIBSYS\n",
      "BIBSYS is an administrative agency set up and organized by the Ministry of Education and Research in Norway. They are a service provider, focusing on the exchange, storage and retrieval of data pertaining to research, teaching and learning – historically ...\n",
      "\n",
      "\u001b[95m\u001b[1m Oxidoreductase 0.0032145168601577442 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Oxidoreductase\n",
      "In biochemistry, an oxidoreductase is an enzyme that catalyzes the transfer of electrons from one molecule, the reductant, also called the electron donor, to another, the oxidant, also called the electron acceptor. This group of enzymes usually utilizes N...\n",
      "\n",
      "\u001b[95m\u001b[1m House of Lords 0.003013869717627307 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/House_of_Lords\n",
      "The House of Lords of the United Kingdom, also known as the House of Peers, is the upper house of the Parliament of the United Kingdom. Like the House of Commons, it meets in the Palace of Westminster.[2] Officially, the full name of the house is the Righ...\n",
      "\n",
      "\u001b[95m\u001b[1m Ethiopia 0.002962138672950028 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Ethiopia\n",
      "Coordinates: 8°N 38°E﻿ / ﻿8°N 38°E﻿ / 8; 38...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = {item[\"url\"]: (item[\"title\"], item[\"snippet\"]) for item in graph_json}\n",
    "search_result = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for url, rank in search_result:\n",
    "    title, snippet = docs[url]\n",
    "    print(\"\\033[95m\\033[1m\", title, rank, \"\\033[0m\")\n",
    "    print(url)\n",
    "    print(snippet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max hits: 0.154157\n",
      "Min hits: 0.000000\n"
     ]
    }
   ],
   "source": [
    "hits = nx.hits(G)\n",
    "print(\"Max hits: %f\" % max(hits[1].values()))\n",
    "print(\"Min hits: %f\" % min(hits[1].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[95m\u001b[1m Integrated Authority File 0.1541569303170916 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Integrated_Authority_File\n",
      "The Integrated Authority File (German: Gemeinsame Normdatei, also known as: Universal Authority File) or GND is an international authority file for the organisation of personal names, subject headings and corporate bodies from catalogues. It is used mainl...\n",
      "\n",
      "\u001b[95m\u001b[1m IMDb 0.058465548616940394 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/IMDb\n",
      "IMDb, also known as Internet Movie Database, is an online database of information related to world films, television programs, home videos and video games, and internet streams, including cast, production crew, personnel and fictional character biographie...\n",
      "\n",
      "\u001b[95m\u001b[1m Bibsys 0.028533962142480322 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/BIBSYS\n",
      "BIBSYS is an administrative agency set up and organized by the Ministry of Education and Research in Norway. They are a service provider, focusing on the exchange, storage and retrieval of data pertaining to research, teaching and learning – historically ...\n",
      "\n",
      "\u001b[95m\u001b[1m Istituto Centrale per il Catalogo Unico 0.007217794772516704 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Istituto_Centrale_per_il_Catalogo_Unico\n",
      "The Central Institute for the Union Catalogue of Italian Libraries and for Bibliographic Information (in Italian: Istituto centrale per il catalogo unico delle biblioteche italiane e per le informazioni bibliografiche) is an Italian government agency that...\n",
      "\n",
      "\u001b[95m\u001b[1m Prignitz 0.0050856463545033885 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Prignitz\n",
      "Prignitz (German pronunciation: [ˈpʁiːɡnɪts]) is a Kreis (district) in the northwestern part of Brandenburg, Germany. Neighboring are (from the north clockwise) the district Ludwigslust-Parchim in Mecklenburg-Western Pomerania, the district Ostprignitz-Ru...\n",
      "\n",
      "\u001b[95m\u001b[1m Template:Brandenburg-geo-stub 0.003980861849334819 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Template:Brandenburg-geo-stub\n",
      "This template is used to identify a Brandenburg location stub. It uses {{asbox}}, which is a meta-template designed to ease the process of creating and maintaining stub templates....\n",
      "\n",
      "\u001b[95m\u001b[1m Lenzen (Elbe) 0.003960093673678384 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Lenzen_(Elbe)\n",
      "Lenzen (Elbe) is a small town in the district of Prignitz, in Brandenburg, Germany. It is part of the Amt Lenzen-Elbtalaue....\n",
      "\n",
      "\u001b[95m\u001b[1m Cumlosen 0.003944119539750405 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Cumlosen\n",
      "Cumlosen is a municipality in the Prignitz district, in Brandenburg, Germany....\n",
      "\n",
      "\u001b[95m\u001b[1m Lanz, Brandenburg 0.003944119539750405 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Lanz,_Brandenburg\n",
      "Lanz is a municipality in the Prignitz district, in Brandenburg, Germany....\n",
      "\n",
      "\u001b[95m\u001b[1m Lenzerwische 0.003944119539750405 \u001b[0m\n",
      "https://en.wikipedia.org/wiki/Lenzerwische\n",
      "Lenzerwische is a municipality in the Prignitz district, in Brandenburg, Germany....\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = {item[\"url\"]: (item[\"title\"], item[\"snippet\"]) for item in graph_json}\n",
    "search_result = sorted(hits[1].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for url, rank in search_result:\n",
    "    title, snippet = docs[url]\n",
    "    print(\"\\033[95m\\033[1m\", title, rank, \"\\033[0m\")\n",
    "    print(url)\n",
    "    print(snippet)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
